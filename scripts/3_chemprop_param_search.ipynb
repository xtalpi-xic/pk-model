{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc8745-b35e-4410-92d6-db91b1d1da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘hyperopt’: File exists\n",
      "mkdir: cannot create directory ‘config’: File exists\n"
     ]
    }
   ],
   "source": [
    "# !mkdir hyperopt config\n",
    "# !mkdir -pv hyperopt/config  hyperopt/hyperopt\n",
    "# conda activate pkmodel\n",
    "# /home/atest/anaconda3/envs/pkmodel/bin/python train_caco2_rdkit_search.py\n",
    "# /home/atest/anaconda3/envs/pkmodel/bin/python /home/zonghu.wang/workspace/pbpk/chemprop/hyperopt/train_cl_morgan_search.py\n",
    "# train_cl_morgan_search.py\n",
    "/home/atest/anaconda3/envs/pkmodel/bin/python train_cl_morgan_search.py\n",
    "/home/atest/anaconda3/envs/pkmodel/bin/python train_caco2_search.py\n",
    "/home/atest/anaconda3/envs/pkmodel/bin/python train_caco2_rdkit_search.py\n",
    "# /home/atest/anaconda3/envs/pkmodel/bin/python /home/zonghu.wang/workspace/pbpk/chemprop/hyperopt/train_caco2_morgan_search.py\n",
    "# /home/atest/anaconda3/envs/pkmodel/bin/python -m pip install git+https://github.com/bp-kelley/descriptastorus\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_fingerprint\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_hyperopt\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_interpret\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_predict\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_train\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/chemprop_web\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/sklearn_predict\n",
    "# #     /data/anaconda3/envs/pkmodel/bin/sklearn_train\n",
    "# #     /data/anaconda3/envs/pkmodel/lib/python3.10/site-packages/chemprop-1.5.2.dist-info/*\n",
    "# #     /data/anaconda3/envs/pkmodel/lib/python3.10/site-packages/chemprop/*\n",
    "# mv  /data/anaconda3/envs/pkmodel/lib/python3.10/site-packages/chemprop-1.5.2.dist-info /data/anaconda3/envs/pkmodel/lib/python3.10/site-packages/chemprop-1.5.2.dist-info-bak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21bcf6e0-f1f3-480f-9fdb-7c11a5532583",
   "metadata": {},
   "source": [
    "# train_fu_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac019dc-b199-4177-a5d5-a52437ad775a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T01:24:47.090543Z",
     "iopub.status.busy": "2023-06-13T01:24:47.089753Z",
     "iopub.status.idle": "2023-06-13T01:24:47.103946Z",
     "shell.execute_reply": "2023-06-13T01:24:47.103011Z",
     "shell.execute_reply.started": "2023-06-13T01:24:47.090486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_fu_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_fu_search.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "# from chemprop.utils import StopWatch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    " \n",
    "from loguru import logger\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','100',\n",
    "                  '--empty_cache',\n",
    " '--search_parameter_keywords','basic',\n",
    "                  '--quiet'] \n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='fup_data_human_2668.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES'\n",
    "target_column= 'fup_log10'\n",
    "data_name='fup_log10_opt'\n",
    "# train(dataset_file,data_name,target_column,smiles_column,batch_size=64,epochs=50)\n",
    "# stopwatch = StopWatch()\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "# stopwatch.stop('hyper search finish: ')\n",
    "\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b3d243f",
   "metadata": {},
   "source": [
    "## train_fu_rdkit_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b826ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_fu_rdkit_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_fu_rdkit_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from chemprop.utils import StopWatch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    " \n",
    "from loguru import logger\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','100',\n",
    "                  '--empty_cache',\n",
    "                          '--no_features_scaling',\n",
    "        '--features_generator','rdkit_2d_normalized',\n",
    "                  '--search_parameter_keywords','batch_size',\n",
    "                  '--quiet'] \n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='fup_data_human_2668.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES_STANDARD'\n",
    "target_column= 'fup_log10'\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    " \n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    " \n",
    "data_name='fup_log10_opt_%s_%s'%(train_param,data_str)\n",
    "# train(dataset_file,data_name,target_column,smiles_column,batch_size=64,epochs=50)\n",
    "stopwatch = StopWatch()\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "stopwatch.stop('hyper search finish: ')\n",
    "\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6772a2b-de78-412c-b5af-75216745b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hyperopt/train_fu_search.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ea68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hyperopt/train_fu_rdkit_search.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1a7b6bc-902b-428c-a3ed-814d257b9f75",
   "metadata": {},
   "source": [
    "# train_cl_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270f607c-536f-481f-8214-34a39ac81a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T01:24:32.972742Z",
     "iopub.status.busy": "2023-06-13T01:24:32.972131Z",
     "iopub.status.idle": "2023-06-13T01:24:33.784758Z",
     "shell.execute_reply": "2023-06-13T01:24:33.783248Z",
     "shell.execute_reply.started": "2023-06-13T01:24:32.972691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_cl_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_cl_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    "from chemprop.utils import StopWatch\n",
    "from loguru import logger\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','50',\n",
    "                  '--gpu','1',\n",
    "                  '--empty_cache',\n",
    "                  '--search_parameter_keywords','batch_size',\n",
    "                  '--quiet']\n",
    "                \n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    " \n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='pk_cl_1215_train_2023_v1.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES_STANDARD'\n",
    "target_column= 'human CL (mL/min/kg)_log10'\n",
    "# target_column= 'human CL (mL/min/kg)_log2'\n",
    "# target_column= 'Y'\n",
    "data_name='pk_cl_1215_train_opt_log10_32_100_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "# train(dataset_file,data_name,target_column,smiles_column,batch_size=64,epochs=50)\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26a6ead2",
   "metadata": {},
   "source": [
    "## train_cl_rdkit_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8910fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_cl_rdkit_a_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_cl_rdkit_a_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    "from chemprop.utils import StopWatch\n",
    "from loguru import logger\n",
    "import os,sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "project_path = Path.cwd().parent\n",
    "log_path = Path(project_path, \"log\")\n",
    "log_time = time.strftime(\"%Y_%m_%d\")\n",
    "logger.remove()\n",
    "logger.add(f'logs/cl_train_epoch_opt_{log_time}.log', format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True,\n",
    "encoding=\"utf-8\", rotation=\"12:00\"\n",
    ")\n",
    "logger.add(sys.stderr, format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True\n",
    ")\n",
    "SEED = 2023\n",
    "EPOCHS = 64\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"rdkit_2d_normalized\"\n",
    "       \n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','60',\n",
    "                  '--empty_cache',\n",
    "                  '--num_workers','32',\n",
    "                  '--gpu','1',\n",
    "                  '--no_features_scaling',\n",
    "                  '--features_generator','rdkit_2d_normalized',\n",
    "                  '--search_parameter_keywords','basic','batch_size',\n",
    "                  '--quiet']\n",
    "        #           \"activation\": hp.choice(\n",
    "        #     \"activation\", options=[\"ReLU\", \"LeakyReLU\", \"PReLU\", \"tanh\", \"SELU\", \"ELU\"]\n",
    "        # ),\n",
    "    #     \"aggregation\": hp.choice(\"aggregation\", options=[\"mean\", \"sum\", \"norm\"]),\n",
    "    #     \"aggregation_norm\": hp.quniform(\"aggregation_norm\", low=1, high=200, q=1),\n",
    "    #     \"batch_size\": hp.quniform(\"batch_size\", low=5, high=200, q=5),\n",
    "    #     \"depth\": hp.quniform(\"depth\", low=2, high=6, q=1),\n",
    "    #     \"dropout\": hp.quniform(\"dropout\", low=0.0, high=0.4, q=0.05),\n",
    "    #     \"ffn_hidden_size\": hp.quniform(\"ffn_hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"ffn_num_layers\": hp.quniform(\"ffn_num_layers\", low=1, high=3, q=1),\n",
    "    #     \"final_lr_ratio\": hp.loguniform(\"final_lr_ratio\", low=np.log(1e-4), high=0.),\n",
    "    #     \"hidden_size\": hp.quniform(\"hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"init_lr_ratio\": hp.loguniform(\"init_lr_ratio\", low=np.log(1e-4), high=0.),\n",
    "    #     \"linked_hidden_size\": hp.quniform(\"linked_hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"max_lr\": hp.loguniform(\"max_lr\", low=np.log(1e-6), high=np.log(1e-2)),\n",
    "    #     \"warmup_epochs\": hp.quniform(\"warmup_epochs\", low=1, high=train_epochs // 2, q=1)\n",
    "    # # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    " \n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='pk_cl_1215_train_2023_v1.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES_STANDARD'\n",
    "target_column= 'human CL (mL/min/kg)_log10'\n",
    "# target_column= 'human CL (mL/min/kg)_log2'\n",
    "# target_column= 'Y'\n",
    "data_name='cl_1215_basic_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "# train(dataset_file,data_name,target_column,smiles_column,batch_size=64,epochs=50)\n",
    "stopwatch = StopWatch()\n",
    "      \n",
    "        \n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "stopwatch.stop('hyper search finish: ')\n",
    "\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n",
    "## #/home/atest/anaconda3/envs/pkmodel/bin/python train_cl_rdkit_search.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6266da4f",
   "metadata": {},
   "source": [
    "## train_cl_morgan_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3a5f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_cl_morgan_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_cl_morgan_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    "from chemprop.utils import StopWatch\n",
    "from loguru import logger\n",
    "import os,sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "project_path = Path.cwd().parent\n",
    "log_path = Path(project_path, \"log\")\n",
    "log_time = time.strftime(\"%Y_%m_%d\")\n",
    "logger.remove()\n",
    "logger.add(f'logs/cl_train_opt_{log_time}.log', format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True,\n",
    "encoding=\"utf-8\", rotation=\"12:00\"\n",
    ")\n",
    "logger.add(sys.stderr, format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True\n",
    ")\n",
    "SEED = 2023\n",
    "EPOCHS = 64\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 50\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"morgan\"\n",
    "       \n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','50',\n",
    "                  '--empty_cache',\n",
    "                  '--num_workers','32',\n",
    "                  '--gpu','0',\n",
    "                  # '--no_features_scaling',\n",
    "                  '--features_generator','morgan',\n",
    "                 '--search_parameter_keywords','basic','batch_size','activation','aggregation',\n",
    "                  '--quiet']\n",
    "        #           \"activation\": hp.choice(\n",
    "        #     \"activation\", options=[\"ReLU\", \"LeakyReLU\", \"PReLU\", \"tanh\", \"SELU\", \"ELU\"]\n",
    "        # ),\n",
    "    #     \"aggregation\": hp.choice(\"aggregation\", options=[\"mean\", \"sum\", \"norm\"]),\n",
    "    #     \"aggregation_norm\": hp.quniform(\"aggregation_norm\", low=1, high=200, q=1),\n",
    "    #     \"batch_size\": hp.quniform(\"batch_size\", low=5, high=200, q=5),\n",
    "    #     \"depth\": hp.quniform(\"depth\", low=2, high=6, q=1),\n",
    "    #     \"dropout\": hp.quniform(\"dropout\", low=0.0, high=0.4, q=0.05),\n",
    "    #     \"ffn_hidden_size\": hp.quniform(\"ffn_hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"ffn_num_layers\": hp.quniform(\"ffn_num_layers\", low=1, high=3, q=1),\n",
    "    #     \"final_lr_ratio\": hp.loguniform(\"final_lr_ratio\", low=np.log(1e-4), high=0.),\n",
    "    #     \"hidden_size\": hp.quniform(\"hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"init_lr_ratio\": hp.loguniform(\"init_lr_ratio\", low=np.log(1e-4), high=0.),\n",
    "    #     \"linked_hidden_size\": hp.quniform(\"linked_hidden_size\", low=300, high=2400, q=100),\n",
    "    #     \"max_lr\": hp.loguniform(\"max_lr\", low=np.log(1e-6), high=np.log(1e-2)),\n",
    "    #     \"warmup_epochs\": hp.quniform(\"warmup_epochs\", low=1, high=train_epochs // 2, q=1)\n",
    "    # # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    " \n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='pk_cl_1215_train_2023.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES'\n",
    "target_column= 'human CL (mL/min/kg)_log10'\n",
    "# target_column= 'human CL (mL/min/kg)_log2'\n",
    "# target_column= 'Y'\n",
    "data_name='pk_cl_1215_log10_16_100_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "# train(dataset_file,data_name,target_column,smiles_column,batch_size=64,epochs=50)\n",
    "stopwatch = StopWatch()\n",
    "      \n",
    "        \n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=16,epochs=100)\n",
    "stopwatch.stop('hyper search finish: ')\n",
    "\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e7b45-5746-439d-b7fe-3fa7a123f03a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T01:28:38.101818Z",
     "iopub.status.busy": "2023-06-13T01:28:38.101046Z",
     "iopub.status.idle": "2023-06-13T01:30:04.581563Z",
     "shell.execute_reply": "2023-06-13T01:30:04.580751Z",
     "shell.execute_reply.started": "2023-06-13T01:28:38.101764Z"
    }
   },
   "outputs": [],
   "source": [
    "%run hyperopt/train_cl_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hyperopt/train_cl_rdkit_search.py\n",
    "#conda activate pkmodel;python train_cl_rdkit_search.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c198eff5-7bf7-4c11-8315-316df1a2032a",
   "metadata": {},
   "source": [
    "# train_caco2_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f17441-3637-4dfb-a603-223e6ebe0f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T01:24:41.837805Z",
     "iopub.status.busy": "2023-06-13T01:24:41.837117Z",
     "iopub.status.idle": "2023-06-13T01:24:41.852030Z",
     "shell.execute_reply": "2023-06-13T01:24:41.851064Z",
     "shell.execute_reply.started": "2023-06-13T01:24:41.837750Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_caco2_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_caco2_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from chemprop.utils import StopWatch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    " \n",
    "from loguru import logger\n",
    "import os,sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "project_path = Path.cwd().parent\n",
    "log_path = Path(project_path, \"log\")\n",
    "log_time = time.strftime(\"%Y_%m_%d\")\n",
    "logger.remove()\n",
    "logger.add(f'logs/caco2_train_ori_opt_{log_time}.log', format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True,\n",
    "encoding=\"utf-8\", rotation=\"12:00\"\n",
    ")\n",
    "logger.add(sys.stderr, format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True\n",
    ")\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                 '--num_iter','60',\n",
    "                  '--empty_cache',\n",
    "                  '--num_workers','8',\n",
    "                  '--gpu','0',\n",
    "                  '--search_parameter_keywords','batch_size',\n",
    "                  '--quiet']\n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='caco2_train_set_6083.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES_STANDARD'\n",
    "# target_column= 'human CL (mL/min/kg)_log10'\n",
    "target_column= 'caco2_log10'\n",
    "# target_column= 'Y'\n",
    "data_name='caco2_log10_ori_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5621a0",
   "metadata": {},
   "source": [
    "## train_caco2_rdkit_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a5bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_caco2_rdkit_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_caco2_rd_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from chemprop.utils import StopWatch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    " \n",
    "from loguru import logger\n",
    "import os,sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "project_path = Path.cwd().parent\n",
    "log_path = Path(project_path, \"log\")\n",
    "log_time = time.strftime(\"%Y_%m_%d\")\n",
    "logger.remove()\n",
    "logger.add(f'logs/caco2_train_opt_{log_time}.log', format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True,\n",
    "encoding=\"utf-8\", rotation=\"12:00\"\n",
    ")\n",
    "logger.add(sys.stderr, format='{time:YYYY-MM-DD HH:mm:ss} | ' \n",
    "                                \"{file} | \"# \n",
    "                               \"{process.name} | \"  # 进程名\n",
    "                               \"{thread.name} | \"  # 进程名\n",
    "                               '{module}.{function}:{line} - {level} -{message}',level=\"INFO\", enqueue=True\n",
    ")\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                 '--num_iter','60',\n",
    "                  '--empty_cache',\n",
    "                  '--num_workers','8',\n",
    "                  '--gpu','0',\n",
    "                  '--no_features_scaling',\n",
    "                  '--features_generator','rdkit_2d_normalized',\n",
    "                  '--search_parameter_keywords','batch_size',\n",
    "                  '--quiet']\n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='caco2_train_set_6083.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES_STANDARD'\n",
    "# target_column= 'human CL (mL/min/kg)_log10'\n",
    "target_column= 'caco2_log10'\n",
    "# target_column= 'Y'\n",
    "data_name='caco2_log10_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93a51e55",
   "metadata": {},
   "source": [
    "## train_caco2_morgan_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa99cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperopt/train_caco2_morgan_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperopt/train_caco2_morgan_search.py\n",
    "import sys\n",
    "sys.path.append(\"/data/project/pbpk/chemprop\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from parameterized import parameterized\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from chemprop.utils import StopWatch\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import chemprop\n",
    "from chemprop.constants import TEST_SCORES_FILE_NAME\n",
    "from chemprop.hyperparameter_optimization import chemprop_hyperopt,hyperopt\n",
    "from chemprop.interpret import chemprop_interpret\n",
    "from chemprop.sklearn_predict import sklearn_predict\n",
    "from chemprop.sklearn_train import sklearn_train\n",
    "from chemprop.train import chemprop_train, chemprop_predict, evaluate_predictions, chemprop_fingerprint\n",
    "from chemprop.web.wsgi import build_app\n",
    "from chemprop.spectra_utils import normalize_spectra, load_phase_mask\n",
    "from chemprop.features import load_features\n",
    "\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    " \n",
    "from loguru import logger\n",
    "SEED = 2023\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_ITER = 2\n",
    "SIZE = 10\n",
    "DEPTH = 3\n",
    "DELTA = 0.025\n",
    "train_param=\"\"\n",
    "def run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=64,epochs=100):\n",
    "    flags: List[str] = None\n",
    "    with TemporaryDirectory() as save_dir:\n",
    "                # Train\n",
    "                config_save_path = os.path.join(save_dir, 'config.json')\n",
    "                dataset_type=\"regression\"\n",
    "             \n",
    "                # Check results\n",
    "\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "            \n",
    "                # parameters = {'depth': (2, 6), 'hidden_size': (300, 2400), 'ffn_hidden_size': (300, 2400), 'ffn_num_layers': (1, 3), 'dropout': (0.0, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 6), 'hidden_size': (300, 1200),\n",
    "                #               'ffn_hidden_size': (300, 1200), 'ffn_num_layers': (1, 3), 'dropout': (0.0,0.2, 0.4)}\n",
    "                # parameters = {'epochs':{50,100,200},'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # parameters = {'batch_size':{16,32,64,128,256,512,1024},'depth': (2, 5), 'dropout': (0.0, 0.2, 0.4)}\n",
    "                # logger.info(\"parameters:%s\"%parameters)\n",
    "            \n",
    "            \n",
    "                raw_args = [\n",
    "                  '--data_path',data_path,\n",
    "                  '--dataset_type', dataset_type,\n",
    "                  '--save_dir', \"hyperopt/\"+data_name,\n",
    "                  '--target_columns', target_column,\n",
    "                  '--smiles_column',smiles_column,\n",
    "                  '--seed', str(SEED),\n",
    "                  '--loss_function','mse',\n",
    "                 # '--checkpoint_dir','ck_dir',\n",
    "                  '--split_type', 'random', # Literal['random', 'scaffold_balanced', 'predetermined', 'crossval', 'cv', 'cv-no-test', 'index_predetermined', 'random_with_repeated_smiles'] = 'random'\n",
    "                  '--split_sizes', '0.8', '0.1', '0.1',\n",
    "                  '--num_folds', str(NUM_FOLDS),\n",
    "                  '--epochs', '%s'%epochs,\n",
    "                  '--metric', 'rmse',\n",
    "                  '--empty_cache',\n",
    "                  '--extra_metrics', 'r2', 'mae',\n",
    "                  '--batch_size','%s'%batch_size,\n",
    "                  '--save_smiles_splits',\n",
    "                  '--config_save_path',\"config/\"+data_name,\n",
    "                  '--num_iter','20',\n",
    "                  '--empty_cache',    \n",
    "                         '--gpu','1',    \n",
    "                  # '--no_features_scaling',\n",
    "                  '--features_generator','morgan',\n",
    "                  '--search_parameter_keywords','basic',\n",
    "                  '--quiet'] \n",
    "    # Hyperopt\n",
    "    # with patch('sys.argv', raw_args):\n",
    "                # command_line = ' '.join(raw_args[:])\n",
    "                # logger.info(f'python hyperparameter_optimization.py {command_line}')\n",
    "\n",
    "                args = chemprop.args.HyperoptArgs().parse_args(raw_args)\n",
    "                logger.info(\"args:%s\"%args)\n",
    "                hyperopt(args=args)\n",
    "\n",
    "                # for parameter, (min_value, max_value) in parameters.items():\n",
    "                #     self.assertTrue(min_value <= config[parameter] <= max_value)\n",
    "    # args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "    # mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)\n",
    "    logger.info(raw_args)\n",
    "    logger.info(\"data_name:%s data_path:%s target_column:%s\"%(data_name,data_path,target_column))\n",
    "    # logger.info(\"mean_score:%s std_score:%s\"%(mean_score,std_score))\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "data_str = now_time.strftime('%Y_%m_%d')\n",
    "dataset_dir= '/data/project/pbpk/data/'\n",
    "dataset_name='caco2_train_set_6083.csv'\n",
    "data_path=dataset_dir+dataset_name\n",
    "logger.info(dataset_dir+dataset_name)\n",
    "smiles_column='SMILES'\n",
    "# target_column= 'human CL (mL/min/kg)_log10'\n",
    "target_column= 'caco2_log10'\n",
    "# target_column= 'Y'\n",
    "data_name='caco2_log10_morgan_%s_%s'%(train_param,data_str)\n",
    "logger.info(\"data_name:%s\"%data_name)\n",
    "stopwatch = StopWatch()\n",
    "run_hyperopt(data_path,data_name,target_column,smiles_column,batch_size=32,epochs=100)\n",
    "stopwatch.stop('hyper search finish: ')\n",
    "# python hyperparameter_optimization.py /data/project/pbpk/data/fup_data_human_2668.csv --dataset_type regression --save_dir result/fup_log10 \n",
    "# --target_columns fup_log10 --smiles_column SMILES --loss_function mse --split_type random --split_sizes 0.8 0.1 0.1 --num_folds 5 --epochs 20 \n",
    "# --batch_size 32 --save_smiles_splits --config_save_path config/fup_log10 --num_iter 10 --empty_cache --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ffad1-b066-4ab1-bcc4-1a40c70b09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hyperopt/train_caco2_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660f8e5-dd47-4398-8eef-c7a0870c3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hyperopt/train_caco2_rdkit_earch.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkmodel",
   "language": "python",
   "name": "pkmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
